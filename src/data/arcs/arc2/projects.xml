<Projects>
  <Process>
    <Essential>Groups choose one of the available briefs and then work on it for this arc</Essential>
    <Encouraged>
      - Groups are randomly assigned one of the available twists.
      - They should attempt to:
        - meet the brief
        - by using the twist as inspiration
    </Encouraged>
  </Process>
  <Briefs>
    <!-- Knowledge & Memory --><Brief>
      <Overview>
        <Name>Knowledge and Memory</Name>

        <Task>Develop a user-facing chatbot that can ingest and summarize long documents or answer questions about them.</Task>

        <Focus>Long-context handling (using models with extended context windows like Anthropic Claude) and summary/"Q and A" prompt patterns. Lays groundwork for retrieval techniques by addressing LLM limits and chunking.</Focus>
      </Overview>

      <Detail>
        <BriefDetail>
          - Develop an AI chatbot that can ingest one or more large documents (such as PDFs, technical documentation, or long texts) and provide useful outputs to a user, such as summaries or question-answering.
          - The chatbot should allow a user to either ask questions about the document’s content or simply request a summary, and the AI will respond based on the document data.
          - Essentially, this project creates a custom “ChatGPT for your documents.”
          - It addresses the common scenario where one needs to quickly extract information from lengthy text.
        </BriefDetail>

        <Objectives>
          <Objective>
           	Handling long context with AI
            - Standard LLMs have context length limits, so feeding an entire long document might be challenging.
            - Participants will learn techniques to work around this.
            - One approach is using models specifically designed for long inputs (e.g. Anthropic’s Claude which offers 100k token context).
            - Another is chunking the document and summarizing or retrieving parts.
            - In either case, they confront the problem of scale – an important concept in AI engineering.
          </Objective>

          <Objective>
           	Summarization and Q&amp;A prompting
            - They will design prompts for summarizing text and for answering questions given some context.
            - This requires a different style than code prompting.
            - They might explore prompt structures like TL;DR summaries, extracting key points, or a QA format (“Using the provided text, answer the question…”).
            - This builds prompt engineering skills for non-code tasks.
          </Objective>

          <Objective>
           	Introduction to Retrieval (light)
            - While full vector database integration is slated for a later arc, this project serves as a gentle introduction.
            - Teams might implement a very basic retrieval by splitting the document into chunks and, when a question is asked, using keyword search or semantic similarity (perhaps via OpenAI’s embeddings endpoint, if they discover it) to find relevant chunks to feed into the prompt.
            - This isn’t mandatory – if using Claude with huge context, they could simply dump everything in – but resource limits might push them toward smarter methods.
            - By grappling with this, they lay groundwork for the dedicated RAG project later.
          </Objective>

          <Objective>
           	User interface considerations
            - Now that the target user is an end-user (who just has a document and questions), teams should consider the UX.
            - Will they build a simple web UI where you upload a doc and chat?
            - Or a command-line where the file path is given?
            - This is a chance for those with front-end skills to shine a bit by making a minimal interface.
            - It’s also an opportunity to discuss the importance of good instructions to users (like telling them what they can ask).
          </Objective>
        </Objectives>
      </Detail>

      <Examples>
        <Example>
          Research Paper Assistant
          - The chatbot loads a long academic paper and allows the user to ask “What are the main findings?” or “How did the authors conduct experiment X?”
          - The AI then answers based on the paper’s content.
        </Example>

        <Example>
          Legal Document Summarizer
          - Upload a lengthy contract or terms-of-service and get a summary in plain language, or ask pointed questions (“What is the cancellation policy in this contract?”).
        </Example>

        <Example>
          Technical Docs Knowledge Base
          - Feed in a large software manual or API documentation.
          - A user (perhaps a developer) can then query, “How do I authenticate with this API?” and the bot will fetch the answer.
        </Example>

        <Example>
          E-book Guide
          - Input an e-book or a long article and chat with the bot to extract key insights or summaries chapter by chapter.
        </Example>
      </Examples>

      <Notes>
      </Notes>
    </Brief>
    <!-- User Agent --><Brief>
      <Overview>
        <Name>User Agent</Name>

        <Task>Build an AI agent that can perform multi-step tasks by invoking external tools or APIs (e.g. an AI that answers user requests by calling weather or search APIs).</Task>

        <Focus>Agent frameworks and function calling. Participants learn how giving an AI tool access extends its capabilities (search the web, do math, etc. via tools ￼). Introduces decision-making by LLMs and the ReAct reasoning paradigm in a practical way.</Focus>
      </Overview>

      <Detail>
        <BriefDetail>
          - Create an AI agent that can autonomously perform a task by utilizing external tools or APIs in addition to just generating text.
          - In practical terms, the agent should take a user request or goal, decide which actions to take (via tools), execute them, and then return a result to the user.
          - This project introduces the concept of AI agents that go beyond static "Q and A" – they can interact with the world (or web/services) to fulfill a user’s intent.
          - The project scope can be adjusted to the tools available, but at minimum each agent must integrate at least one external non-LLM service (API, database, or even a computational tool like a calculator).
        </BriefDetail>

        <Objectives>
          <Objective>
            Understanding AI agents and tool use
            Participants learn why giving tools to LLMs is powerful – it extends what the AI can do (e.g., fetch real-time information, perform calculations) ￼. They also learn how to implement this. This could be via a library (like LangChain, which provides agent abstractions) or manually using OpenAI’s function calling (where the AI can request a function and you fulfill it). Either way, they grasp the loop of LLM decides action → action executed → result fed back → LLM continues.
          </Objective>

          <Objective>
            Multi-step reasoning (ReAct framework)
            To make an agent effective, often we employ a prompt that encourages reasoning and action step by step. Participants will practice writing such prompts or configuring an agent framework. For example, they may use the ReAct prompting technique (Reason+Act) that lets the model think in steps (“I should do X next.”) ￼. This skill is crucial for building more complex AI workflows reliably.
          </Objective>

          <Objective>
            Integration of APIs and handling real data
            Teams will gain experience working with at least one third-party API or data source (beyond AI APIs). This could involve reading API docs, obtaining API keys for, say, a weather or search service, and parsing the responses. It’s a practical skill to combine AI with conventional software APIs. They also must handle errors – e.g., what if the API call fails or returns nothing? The agent should handle it gracefully, which introduces robustness considerations.
          </Objective>

          <Objective>
            Balancing autonomy and control
            Building an autonomous agent leads to questions of how much freedom the AI should have. Participants naturally will think about constraints (e.g., limiting how many times it can call a tool to avoid infinite loops ￼, or sandboxing what it can do). This raises awareness of AI safety considerations in a hands-on way (like preventing an agent from doing something harmful or wasting resources). We likely won’t hit serious safety issues in our simple projects, but the concept of curbing an AI’s actions is introduced.
          </Objective>
        </Objectives>
      </Detail>

      <Examples>
        <Example>
          Web Research Agent
          The user asks a question like “What were the key outcomes of the latest climate summit?” The agent then uses a search tool (calls a search API or scrapes a web page) to find information and then summarizes the answer. This involves a search tool and perhaps a browser-fetch tool.
        </Example>

        <Example>
          Personal Assistant Agent
          A user can give a command “Schedule a meeting with Alice next week” – the agent could check a (mock) calendar or scheduling API, pick a time, maybe send an email or create an event (this might be simulated if we don’t want to use real email APIs, but they could integrate Google Calendar API if they wish). This showcases multi-step action: check schedules → compose response.
        </Example>

        <Example>
       	  DevOps/Test Agent
          Though participants aren’t big on DevOps, some may still find this cool – e.g. an agent that, when given a GitHub issue about a bug, automatically spins up a test environment or runs a test suite to reproduce it (using say a Docker tool or CI API) then reports back. This is more complex and might be too heavy for this timeframe, but a scaled-down version could be interesting (like it can run a specific script and return output).
        </Example>

        <Example>
         	Data Analysis Agent
          The user provides a dataset or a link to one and asks “What are the insights?” The agent could then load the data (using a Python tool/pandas via some bridging code), do a simple analysis (like compute averages), and then have the LLM explain the results. This chain shows using a Python tool for computation.
        </Example>

        <Example>
         	Game-playing Agent
          For fun, an agent that plays a text-based game or solves puzzles by deciding on actions. For example, a “Maze solver AI” that can call a move(direction) tool to navigate a maze and uses reasoning to figure out the path. This is more of an entertainment option but teaches the same principle of thought-action loops.
        </Example>
      </Examples>

      <Notes>
        <Note>
          Given the diversity of possible tools, each team might pick a very different approach, which is great for breadth of learning. However, we’ll ensure everyone implements at least a simple tool like a calculator or search in their agent so the core concept is covered. (One trivial built-in tool is a calculator – many LangChain tutorials start with giving the LLM a calculator tool so it can answer math questions it normally would get wrong ￼.)
        </Note>

        <Note>
          During building, groups who use Python might spin up a small Flask server or Jupyter environment to host their agent logic, whereas others using Node and OpenAI functions might keep it all in one script. By Week 12, we expect working demos where a user query leads the agent to take at least one tool action and then return an answer. The presentations will be exciting because they show AI systems doing things like retrieving live info or interacting with user data – very cutting-edge stuff that moves beyond static Q&amp;A. We will likely see some agents succeed impressively and others struggle (for instance, the web research agent might sometimes fetch irrelevant info). This is valuable for learning; it will prompt discussions on how to improve tool use, and highlight that giving AI tools is powerful but requires careful design (thus leading well into evaluation and grounding topics in later projects).
        </Note>

        <Note>
          By completing this project, participants step firmly into the realm of AI engineering (not just calling an API to get a chat answer, but orchestrating AI reasoning and actions). This skillset is highly sought after, as evidenced by many frameworks and hackathons around AI agents ￼. The project also underscores the importance of a human-in-the-loop mindset.
        </Note>
      </Notes>
    </Brief>
  </Briefs>
  <Twists>
    <!-- It's Always Watching --> <Twist>
      <Name>It's Always Watching...</Name>
      <Task>Build something that gives AI persistent, evolving context.</Task>
      <Examples>
        <Example>An interaction environment/frontend that remembers your interaction patterns across sessions</Example>
        <Example>A documentation system that learns from your team's questions</Example>
        <Example>An assistant that builds institutional knowledge</Example>
      </Examples>
		</Twist>
  </Twists>
</Projects>

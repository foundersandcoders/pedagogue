<Research>
  <Overview>
    These are topics that might be useful foci for the initial research session at the beginning of the module.
  </Overview>

	<PrimaryTopics>
   	<!-- Long Context/Embeddings Models -->
    <PrimaryTopic>
      1. Long Context Models
        > Find out what models are available that can handle big inputs.
        - For example:
          - Claude’s 100k context model (Claude Instant or Claude 2)
          - GPT-4 32k context
        - How do these work, what are their limits, and are they accessible with our keys?
        - One group could specifically research Anthropic Claude’s long context and maybe come with an example of its use.
        - The Google Cloud RAG page even mentions using Gemini’s long context window for grounding docs
      2. Embedding Models &amp; Costs
        > Research what embedding model to use for text
        - How much does it cost?
        - How fast is it to embed a typical document?
        - How do you choose the embedding dimensionality or model, and does it matter for accuracy?
        - Investigate open-source embedding models
          - SBERT
          - InstructorXL
    </PrimaryTopic>

   	<!-- Handling Inputs -->
    <PrimaryTopic>
      1. Text Chunking and Summarization
        > Techniques to summarize or chunk documents
        - For instance:
          - recursive summarization (summarize each section then summarize the summaries)
          - splitting by headings
        - Also, how to avoid losing key info in summarization.
        - They might find some best practices or libraries, like:
          - Python’s NLTK to break text
          - Node’s similar packages
      2. Embeddings &amp; Similarity Search
        > An intro to the concept of embeddings as a way to find related text
        - Perhaps one group experiments with OpenAI’s text-embedding API in a script
          - convert chunks of a sample text to embeddings and see how to find which chunks best match a query
        - This is a precursor to vector DB but can be done with basic array similarity if needed.
      3. Tools for PDF/Text processing
        > Identify libraries to extract text from PDFs or websites
        - for example:
          - Python’s PyPDF
          - JavaScript PDF parsers
        - If the documents are HTML or MD, find ways to parse them
        - Essentially, ensure teams know how to get raw text from the input documents for the AI to consume.
    </PrimaryTopic>

   	<!-- Handling Code -->
    <PrimaryTopic>
      1. AI for Code Review &amp; Quality
        > How are AI models being used to review code?
        - Investigate examples like:
          - GitHub’s PR review assistant
          - AWS CodeGuru reviewer
        - What types of suggestions do they provide?
          - bug detection
          - style improvements
          - performance suggestions
        - Any success stats or case studies
          - e.g. reduction in review time
      2. Handling Code Input for LLMs
        > Best practices for feeding code into an LLM prompt
        - How to format the prompt when giving multiple files or a diff?
        - Research techniques like...
          - summarizing or truncating long code
          - focusing the AI on certain sections (e.g. “Here is the diff of this PR…”)
        - Also consider token limits &amp; how to stay within them for large codebases
          - analyzing only changed code
          - using a brief high-level summary
      3. AI-Generated Tests or Docs
        > If teams opt for test generation or documentation, research how LLMs can create these
        - For test generation: how to prompt for a unit test given a function’s code or spec
        - For documentation: using AI to generate a README or update a changelog.
        - Find any tools or papers on these topics to gauge feasibility and best approaches.
    </PrimaryTopic>

   	<!-- CI/CD -->
    <PrimaryTopic>
      1. Security and Privacy in CI AI
        > Discuss the implications of sending your code to an external API (like OpenAI) during CI
        - What do OpenAI/Anthropic’s terms say about data usage?
        - Are there safer alternatives
          - we will explore self-hosted models later in the course
      2. Integrating with GitHub Actions
        > Learn how GitHub Actions (or similar CI services) can run custom code
        - Specifically...
          1. how to write a workflow that triggers on events (like pull_request opened)
          2. calls a script
        - Are there existing Action templates for...
          - calling APIs?
          - posting comments?
        - Outline the basic structure of an Action YAML and the environment it runs in, e.g.
          - runners
          - using Node or Python in actions
    </PrimaryTopic>

   	<!-- Knowledge -->
    <PrimaryTopic>
      1. Retrieval-Augmented Generation (RAG) 101
        > Understand the concept of RAG
        - Break down the typical steps
          1. chunking documents
          2. embedding them into vectors
          3. storing in a vector database
          4. retrieving top relevant chunks for a query
          4. injecting into the prompt
        - What are the benefits of this approach compared to fine-tuning a model on the data?
          - e.g. RAG can easily update info and reduce hallucinations
        - Look for any success metrics or examples of RAG in action
          - like how much it improved accuracy in open-domain QA
      2. Knowledge Frameworks
        > Explore frameworks like LangChain or LlamaIndex which abstract a lot of the RAG process
        - What features do they provide?
          - document loaders
          - index builders
          - ready-made chat chain logic
        - Examine vector database options
          - FAISS
          - Pinecone
          - Weaviate
          - etc.
        – How do they work?
        - What are their free tiers or ease-of-use considerations?
    </PrimaryTopic>

   	<!-- Context & Control -->
    <PrimaryTopic>
      1. Preventing Hallucinations and Ensuring Accuracy
        > Strategies to make the bot cite sources or indicate uncertainty
        - Instructing the model “If you don’t find an answer in the provided documents, say you don’t know.”
        - Find out if certain prompt formats yield better factual accuracy (like asking the model to list sources).
        - Consider evaluation: how will we know if our bot’s answer was correct or just plausible?
        - Look up any methods for QA evaluation or human-in-the-loop verification.
      2. Maintaining Chatbot Context
        > Techniques for a chatbot to handle long conversations
        - One approach is conversation summarization
          – periodically summarize earlier chat history
          - use that summary for context
        - Another is persistent memory via a database
          - storing facts learned about the user
        - Research how ChatGPT or Claude handle large contexts
          - e.g. window of 100k tokens in Claude 2
        - What are best practices to avoid the bot “forgetting” important details or repeating itself?
    </PrimaryTopic>
  </PrimaryTopics>
	<StretchTopics>
	  <StretchTopic>Advanced Tools: Look into how agents can use tools like a Python REPL (executing code) or even image recognition.</StretchTopic>
		<StretchTopic>DevOps Automation Trends: Any emerging concept of “NoOps” with AI or using AI for infrastructure as code?</StretchTopic>
		<StretchTopic>Domain-Specific Challenges</StretchTopic>
		<StretchTopic>Graph databases for knowledge representation</StretchTopic>
		<StretchTopic>Fine-tuning vs few-shot learning</StretchTopic>
		<StretchTopic>Long-term memory architectures for LLMs</StretchTopic>
		<StretchTopic>stateful conversations</StretchTopic>
		<StretchTopic>Open source model landscape (Llama, Mistral, Qwen)</StretchTopic>
		<StretchTopic>Edge deployment strategies</StretchTopic>
		<StretchTopic>Local inference</StretchTopic>
		<StretchTopic>quantisation trade-offs</StretchTopic>
	</StretchTopics>
</Research>

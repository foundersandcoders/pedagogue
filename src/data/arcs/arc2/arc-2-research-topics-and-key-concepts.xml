<research-topics>
	<primary-topics>
   	<primary-topic-1>
      Name:
      1.
    </primary-topic-1>
  </primary-topics>

	<stretch-topics>
    1.
	</stretch-topics>
</research-topics>

<research-topics>
		<primary>
				•	Long Context Models: Find out what models are available that can handle big inputs. For example, Claude’s 100k context model (Claude Instant or Claude 2) and GPT-4 32k context. How do these work, what are their limits, and are they accessible with our keys? One group could specifically research Anthropic Claude’s long context and maybe come with an example of its use. ￼ (The Google Cloud RAG page even mentions using Gemini’s long context window for grounding docs ￼.)
     	•	Text Chunking and Summarization: Techniques to summarize or chunk documents. For instance, recursive summarization (summarize each section then summarize the summaries), or splitting by headings. Also, how to avoid losing key info in summarization. They might find some best practices or libraries (like Python’s NLTK to break text, or Node’s similar packages).
     	•	Embeddings & Similarity Search: An intro to the concept of embeddings as a way to find related text. Perhaps one group experiments with OpenAI’s text-embedding API in a script: convert chunks of a sample text to embeddings and see how to find which chunks best match a query. This is a precursor to vector DB but can be done with basic array similarity if needed.
     	•	Accuracy and Hallucination: Investigate the issue of AI hallucination and why simply asking a model about a document might yield made-up answers if not properly grounded. Find examples of failures where models confidently state wrong info. This motivates why doing things like citing the source text is important. We likely won’t fully solve hallucinations here, but raising awareness sets the stage for Project 5.
     	•	Tools for PDF/Text processing: Identify libraries to extract text from PDFs or websites (for example, Python’s PyPDF or JavaScript PDF parsers). If the documents are HTML or MD, find ways to parse them. Essentially, ensure teams know how to get raw text from the input documents for the AI to consume.
     	•	AI for Code Review & Quality: How are AI models being used to review code? Investigate examples like GitHub’s PR review assistant or AWS CodeGuru reviewer. What types of suggestions do they provide (bug detection, style improvements, performance suggestions)? Any success stats or case studies (e.g. reduction in review time)? ￼ ￼
•	Integrating with GitHub Actions: Learn how GitHub Actions (or similar CI services) can run custom code. Specifically, how to write a workflow that triggers on events (like pull_request opened) and then calls a script. Are there existing Action templates for calling APIs or posting comments? Outline the basic structure of an Action YAML and the environment it runs in (e.g. runners, using Node or Python in actions).
•	Handling Code Input for LLMs: Best practices for feeding code into an LLM prompt. How to format the prompt when giving multiple files or a diff? Research techniques like summarizing or truncating long code, or focusing the AI on certain sections (e.g. “Here is the diff of this PR…”). Also consider token limits – how to stay within them for large codebases (maybe by analyzing only changed code or using a brief high-level summary).
•	AI-Generated Tests or Docs: If teams opt for test generation or documentation, research how LLMs can create these. For test generation: how to prompt for a unit test given a function’s code or spec. For documentation: using AI to generate a README or update a changelog. Find any tools or papers on these topics to gauge feasibility and best approaches.
•	Security and Privacy in CI AI: Discuss the implications of sending your code to an external API (like OpenAI) during CI. What do OpenAI/Anthropic’s terms say about data usage? ￼ Are there safer alternatives (like self-hosted models – which we will actually explore later in the course)? This might foreshadow why open-source models could be useful when dealing with sensitive code.
•	DevOps Automation Trends: (Optional) Broader look at how AI is impacting DevOps. Any emerging concept of “NoOps” with AI or using AI for infrastructure as code? This can be exploratory to inspire creative project angles (e.g. AI suggesting CI pipeline optimizations).
		•	LangChain or Agent Libraries: One group might dive into LangChain (Python or JS) to understand how it facilitates agents and what tools exist out-of-the-box. They could demo a quick LangChain agent that uses a math tool, for instance, to show others.
		•	OpenAI Function Calling: Another group can focus on OpenAI’s newer capability where you can define functions that the model can call. They’ll research how to set up a system message with function specs, and how the model responds with a function call, etc. This is an alternative to a full agent framework and can be easier to control in some cases.
		•	APIs for Tools: Each group will likely have to research a specific API relevant to their idea (e.g. Google Custom Search API, OpenWeatherMap for weather, Twilio for SMS, etc.). In the research session, they can share the basics of how to use those APIs. Even though not everyone will use every API, hearing about a couple gives insight into the general process of calling external services.
		•	Agent Failure Modes & Safety: Research famous cases or examples of where autonomous AI agents go wrong. For example, the early experiences with Auto-GPT getting stuck in loops or doing pointless tasks could be anecdotal evidence to discuss. Another aspect: if someone finds material on the “ReAct” paper ￼ or similar, they can share how structuring an agent’s reasoning helps. Also, discuss the importance of setting limits (max iterations, etc. ￼) to prevent runaway behavior.
		•
		•	Retrieval-Augmented Generation (RAG) 101: Understand the concept of RAG. Break down the typical steps: chunking documents, embedding them into vectors, storing in a vector database, retrieving top relevant chunks for a query, and injecting into the prompt ￼. What are the benefits of this approach compared to fine-tuning a model on the data? (E.g. RAG can easily update info and reduce hallucinations ￼.) Look for any success metrics or examples of RAG in action (like how much it improved accuracy in open-domain QA).
				•	Tools for Building Knowledge Bots: Explore frameworks like LangChain or LlamaIndex which abstract a lot of the RAG process ￼. What features do they provide (e.g. document loaders, index builders, ready-made chat chain logic)? Also, examine vector database options (FAISS, Pinecone, Weaviate, etc.) – how do they work and what are their free tiers or ease-of-use considerations?
				•	Embedding Models & Costs: Research what embedding model to use for text (OpenAI’s text-embedding-ada, or open-source alternatives). How much does it cost or how fast is it to embed a typical document? Also, how do you choose the embedding dimensionality or model, and does it matter for accuracy? Investigate if open-source embedding models (e.g. SBERT or InstructorXL) are viable if a team wants to avoid API costs.
				•	Maintaining Chatbot Context: Techniques for a chatbot to handle long conversations. One approach is conversation summarization – periodically summarize earlier chat history and use that summary for context. Another is persistent memory via a database (storing facts learned about the user). Research how ChatGPT or Claude handle large contexts (e.g. window of 100k tokens in Claude 2). What are best practices to avoid the bot “forgetting” important details or repeating itself?
				•	Preventing Hallucinations and Ensuring Accuracy: Strategies to make the bot cite sources or indicate uncertainty. For example, instructing the model “If you don’t find an answer in the provided documents, say you don’t know.” Find out if certain prompt formats yield better factual accuracy (like asking the model to list sources). Also, consider evaluation: how will we know if our bot’s answer was correct or just plausible? Look up any methods for QA evaluation or human-in-the-loop verification.
				•	Domain-Specific Challenges: (Optional) If teams choose specialized domains (like a medical info bot or legal doc bot), research the additional responsibilities (ensuring up-to-date regulations, avoiding advice beyond the docs, etc.). Also, any known issues such as biases in embeddings for certain languages or topics.
		</primary>

		<stretch>
				- Advanced Tools
						|- If time, look into how agents can use tools like a Python REPL (executing code) or even image recognition. This might be beyond our project scope, but it can inspire those who want to push boundaries, perhaps to incorporate a small coding tool in their agent.
		</stretch>
</research-topics>

<inbox>
		|- Long-term memory architectures for LLMs
		|- Graph databases for knowledge representation
		|- Fine-tuning vs few-shot learning
		|- Conversation summarisation techniques
		|- Context compression strategies
		|- Memory systems
		|- knowledge graphs
		|- stateful conversations
		|- Ollama, vLLM, and local inference optimisation
		|- Model quantisation (GGUF, GPTQ, AWQ)
		|- Open source model landscape (Llama, Mistral, Qwen)
		|- Edge deployment strategies
		|- Privacy-preserving ML techniques
		|- Local inference
		|- model selection
		|- quantisation trade-offs
</inbox>

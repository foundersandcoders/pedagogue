<research-topics>
	<primary-topics>
   	<primary-topic>
      1. Long Context Models
        > Find out what models are available that can handle big inputs.
        - For example:
          - Claude’s 100k context model (Claude Instant or Claude 2)
          - GPT-4 32k context
        - How do these work, what are their limits, and are they accessible with our keys?
        - One group could specifically research Anthropic Claude’s long context and maybe come with an example of its use.
        - The Google Cloud RAG page even mentions using Gemini’s long context window for grounding docs
      2. Embedding Models &amp; Costs
        > Research what embedding model to use for text
        - How much does it cost?
        - How fast is it to embed a typical document?
        - How do you choose the embedding dimensionality or model, and does it matter for accuracy?
        - Investigate open-source embedding models
          - SBERT
          - InstructorXL
    </primary-topic>

   	<primary-topic>
      1. Text Chunking and Summarization
        > Techniques to summarize or chunk documents
        - For instance:
          - recursive summarization (summarize each section then summarize the summaries)
          - splitting by headings
        - Also, how to avoid losing key info in summarization.
        - They might find some best practices or libraries, like:
          - Python’s NLTK to break text
          - Node’s similar packages
      2. Embeddings &amp; Similarity Search
        > An intro to the concept of embeddings as a way to find related text
        - Perhaps one group experiments with OpenAI’s text-embedding API in a script
          - convert chunks of a sample text to embeddings and see how to find which chunks best match a query
        - This is a precursor to vector DB but can be done with basic array similarity if needed.
      3. Tools for PDF/Text processing
        > Identify libraries to extract text from PDFs or websites
        - for example:
          - Python’s PyPDF
          - JavaScript PDF parsers
        - If the documents are HTML or MD, find ways to parse them
        - Essentially, ensure teams know how to get raw text from the input documents for the AI to consume.
    </primary-topic>

   	<primary-topic>
      1. AI for Code Review &amp; Quality
        > How are AI models being used to review code?
        - Investigate examples like:
          - GitHub’s PR review assistant
          - AWS CodeGuru reviewer
        - What types of suggestions do they provide?
          - bug detection
          - style improvements
          - performance suggestions
        - Any success stats or case studies
          - e.g. reduction in review time
      2. Handling Code Input for LLMs
        > Best practices for feeding code into an LLM prompt
        - How to format the prompt when giving multiple files or a diff?
        - Research techniques like...
          - summarizing or truncating long code
          - focusing the AI on certain sections (e.g. “Here is the diff of this PR…”)
        - Also consider token limits &amp; how to stay within them for large codebases
          - analyzing only changed code
          - using a brief high-level summary
      3. AI-Generated Tests or Docs
        > If teams opt for test generation or documentation, research how LLMs can create these
        - For test generation: how to prompt for a unit test given a function’s code or spec
        - For documentation: using AI to generate a README or update a changelog.
        - Find any tools or papers on these topics to gauge feasibility and best approaches.
    </primary-topic>

   	<primary-topic>
      1. Security and Privacy in CI AI
        > Discuss the implications of sending your code to an external API (like OpenAI) during CI
        - What do OpenAI/Anthropic’s terms say about data usage?
        - Are there safer alternatives
          - we will explore self-hosted models later in the course
      2. Integrating with GitHub Actions
        > Learn how GitHub Actions (or similar CI services) can run custom code
        - Specifically...
          1. how to write a workflow that triggers on events (like pull_request opened)
          2. calls a script
        - Are there existing Action templates for...
          - calling APIs?
          - posting comments?
        - Outline the basic structure of an Action YAML and the environment it runs in, e.g.
          - runners
          - using Node or Python in actions
    </primary-topic>

   	<primary-topic>
      1. Retrieval-Augmented Generation (RAG) 101
        > Understand the concept of RAG
        - Break down the typical steps
          1. chunking documents
          2. embedding them into vectors
          3. storing in a vector database
          4. retrieving top relevant chunks for a query
          4. injecting into the prompt
        - What are the benefits of this approach compared to fine-tuning a model on the data?
          - e.g. RAG can easily update info and reduce hallucinations
        - Look for any success metrics or examples of RAG in action
          - like how much it improved accuracy in open-domain QA
      2. Tools for Building Knowledge Bots
        > Explore frameworks like LangChain or LlamaIndex which abstract a lot of the RAG process
        - What features do they provide?
          - document loaders
          - index builders
          - ready-made chat chain logic
        - Examine vector database options
          - FAISS
          - Pinecone
          - Weaviate
          - etc.
        – How do they work?
        - What are their free tiers or ease-of-use considerations?
    </primary-topic>

   	<primary-topic>
      1. Preventing Hallucinations and Ensuring Accuracy
        > Strategies to make the bot cite sources or indicate uncertainty
        - Instructing the model “If you don’t find an answer in the provided documents, say you don’t know.”
        - Find out if certain prompt formats yield better factual accuracy (like asking the model to list sources).
        - Consider evaluation: how will we know if our bot’s answer was correct or just plausible?
        - Look up any methods for QA evaluation or human-in-the-loop verification.
      2. Maintaining Chatbot Context
        > Techniques for a chatbot to handle long conversations
        - One approach is conversation summarization
          – periodically summarize earlier chat history
          - use that summary for context
        - Another is persistent memory via a database
          - storing facts learned about the user
        - Research how ChatGPT or Claude handle large contexts
          - e.g. window of 100k tokens in Claude 2
        - What are best practices to avoid the bot “forgetting” important details or repeating itself?
    </primary-topic>

   	<primary-topic>
    </primary-topic>
  </primary-topics>

	<stretch-topics>
	  <stretch-topic>
      Advanced Tools
      > Look into how agents can use tools like a Python REPL (executing code) or even image recognition. This might be beyond our project scope, but it can inspire those who want to push boundaries, perhaps to incorporate a small coding tool in their agent.
    </stretch-topic>

   	<stretch-topic>
      DevOps Automation Trends
      > Broader look at how AI is impacting DevOps. Any emerging concept of “NoOps” with AI or using AI for infrastructure as code? This can be exploratory to inspire creative project angles (e.g. AI suggesting CI pipeline optimizations).
    </stretch-topic>

   	<stretch-topic>
      Domain-Specific Challenges
      > If teams choose specialized domains (like a medical info bot or legal doc bot), research the additional responsibilities (ensuring up-to-date regulations, avoiding advice beyond the docs, etc.). Also, any known issues such as biases in embeddings for certain languages or topics.
    </stretch-topic>

   	<stretch-topic>Graph databases for knowledge representation</stretch-topic>
   	<stretch-topic>Fine-tuning vs few-shot learning</stretch-topic>
   	<stretch-topic>Long-term memory architectures for LLMs</stretch-topic>
    <stretch-topic>stateful conversations</stretch-topic>
   	<stretch-topic>Open source model landscape (Llama, Mistral, Qwen)</stretch-topic>
   	<stretch-topic>Edge deployment strategies</stretch-topic>
   	<stretch-topic>Local inference</stretch-topic>
   	<stretch-topic>quantisation trade-offs</stretch-topic>
	</stretch-topics>
</research-topics>

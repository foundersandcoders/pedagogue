<research-topics>
	<primary-topics>
   	<primary-topic-1>
      Name:
      1.
    </primary-topic-1>
  </primary-topics>

	<stretch-topics>
    1.
	</stretch-topics>
</research-topics>

<research-topics>
		<primary>
				- RAG Concepts and Case Studies: Find explanations of why RAG is useful and how companies use it (e.g. how Bing Search or enterprise chatbots use retrieval to give up-to-date info). Google’s documentation or other blog posts could be referenced to highlight that RAG “combines LLMs with external knowledge to get more accurate, up-to-date answers” ￼.
				-	Vector Databases: Look into at least one or two vector DB options. One group might try Chroma (since it’s open source and easy to run), another might experiment with Pinecone’s free tier, etc. They’ll report on how to index data and query it. Possibly even demonstrate a tiny example (e.g. indexing 5 sentences and querying one).
				-	Embedding Models: Research what embedding model to use for the text. Likely OpenAI’s text-embedding-ada-002 is a straightforward choice via API. But if someone wants to avoid API, they might find a locally running embedding model. Discuss the trade-offs (OpenAI’s embeddings are high-quality but cost per call, local models might be free but less performant). Also, the concept of dimensionality and how semantic similarity works is worth covering conceptually.
				-	Prompting for QA with context: How to format the prompt when you have retrieved documents. E.g., strategies like “Answer the question using only the following context…” and perhaps instructing the model to say if it doesn’t know or if info wasn’t found. Also how to ask it to provide the source (some prompt templates explicitly say: “Include a reference to the document in your answer”). The groups can find community prompts or literature on this.
				-	Evaluation of Answers: Since this project strives for factual accuracy, how will we evaluate the chatbot’s answers? This could segue into introducing the idea of an evaluation set or some unit tests for the chatbot (not too formal, but e.g. prepare a couple of sample questions we know answers to and see if the bot gets them). Also mention metrics like precision of retrieval or the concept of groundedness ￼ measured by some AI evaluators. While we won’t implement a full evaluation pipeline, awareness of how one might systematically test an AI QA system is valuable (especially as an industry practice).
				•	OpenAI Function Calling: Understand OpenAI’s function calling feature in the Chat API ￼. How do developers specify functions (with name and parameters) and how the model decides to output a JSON to call them? Look at examples – e.g. calling a weather API, database query function. What are the best practices to get reliable function calls (like clearly defining when to use which function in the system prompt)?
				•	Agent Frameworks: Explore libraries like LangChain agents or Microsoft’s Semantic Kernel, etc. What do they provide? For instance, LangChain can set up tools and uses a reasoning loop (often implementing the ReAct framework). How do these agents decide on multiple steps (think: plan, act, observe, repeat)? Share a simple agent example from documentation (like a Python snippet that uses an LLM to do a Google search, then uses results to answer).
				•	Case Studies of Autonomous Agents: Research experiments like AutoGPT or BabyAGI. What kinds of tasks were they able to automate? What challenges were encountered (getting stuck in loops, irrelevant actions)? Also, any metrics or anecdotes on how effective or inefficient they are (for example, these agents sometimes make mistakes or take many steps to do something simple). This will temper expectations and inspire ideas on what manageable scope our agents should have (likely one or two tool uses, not infinite loops). ￼ ￼
				•	Designing Tool APIs: Suppose we want our AI to do specific tasks – how to choose and design the tools for it? Research principles for tool selection: e.g. giving an AI a calculator, a web search, and a knowledge lookup might be common. If someone finds a resource on Model-Compute-Pipeline (MCP) (an emerging standard for connecting LLMs to tools) ￼ ￼, summarize what it says about structuring these interactions. Think also about errors: what if the tool returns nothing useful – how should the AI respond?
				•	Ensuring Safe and Relevant Actions: Investigate how to keep the agent from doing unwanted things. One aspect is constraining the tools available (our agent can’t randomly execute any code – only call the functions we expose). But even with limited tools, it might use them in silly ways. Are there known strategies to keep the agent on track? (For example, requiring the agent to explain why it’s calling a function, or having a time-out if too many steps.) Also discuss ethical boundaries: if hooking to external APIs, ensure no privacy breach (e.g. if calling a user calendar, the data stays local).
				•
		</primary>

		<stretch>
				- Multi-turn and Memory in Agents
						|- If the agent carries state (like results from previous function calls), how is that managed? Research if agent frameworks store intermediate results or if the model remembers it through the conversation context. How could an agent handle a multi-step query from a user (like “Find me a restaurant and book a table” requires search then booking)? Understanding this can help in designing our agent’s prompt flow.
		</stretch>
</research-topics>

<inbox>
		- Advanced multi-agent frameworks (AutoGen, CrewAI)
		- Agent communication protocols & negotiation
		- Mixture of experts approaches
		- Task decomposition strategies
		- Consensus mechanisms in AI systems
		- Emergent behaviours in agent collectives
		- Agent orchestration
		- emergent intelligence
		- distributed problem-solving
</inbox>

<purpose>
  <overview>
  This app is designed to facilitate the creation of new module specifications for a peer-led AI Engineering Course. It allows the temporary administration council to upload the `<arc></arc>` and `<next-step></next-step>` .xml chunks as files along with the following prompt:
  </overview>

  <prompt>
    <between-arcs>
  		- Arc council runs an extended thinking task in an LLM (e.g. ChatGPT deep research) or, ideally, in the Pedagogue app.
  		- They upload the `<arc></arc>` and `<next-step></next-step>` .xml chunks as files along with the following prompt
    </between-arcs>

    <scenario>
  		We are the temporary administration council of a peer-led AI Engineering Course. Our curriculum was created in September 2025and we need to ensure the next module and project brief is up to date when we start it in {DATE}.
    </scenario>

    <task>
  		1. Analyse the attached file
  		2. Ask any clarifying questions you need in order to complete step 3
  		3. Confirm you understand the module's learning outcomes and the course's overall focus
  		4. Conduct deep research and think deeply about whether the module still represents an up to date approach
  		5. Ask any clarifying questions you need in order to complete step 6
  		6. Create a new module specification in exactly the same format as the file we have uploaded
    </task>
  </prompt>

  <example-inputs>
    <arc>
        <option-1>
    		<overview>
    				- Build a personal AI-assisted development tool that helps with coding tasks.
    				- Focus: Using large-language-model (LLM) APIs for developer productivity (e.g. code generation, explanation).
    				- Introduces prompt engineering fundamentals and structured AI outputs in a familiar JS environment.
    		</overview>

    		<brief>
    				- Create a personal AI-powered coding assistant that helps developers with one or two common programming tasks.
    				- This could be a command-line tool, a chat interface, or an editor plugin
    				- For example, your tool might...
    						|- generate boilerplate code from a short description
    						|- explain what a piece of code does
    						|- suggest improvements to a function
    						|- answer questions about a programming error
    				- The key is to leverage an AI model (via API) to augment the coding process.
    		</brief>

    		<objectives>
    				- Call AI APIs (OpenAI, Anthropic, etc.) from an application
    						|– setting up API keys, making requests, and handling responses.
             	- Practice prompt engineering focused on code tasks
   	       	|- crafting effective prompts to get desired outputs
   	       	|- This might include using OpenAI’s structured response features
             	- Understand the capabilities and limits of code-focused LLMs
             	- Gain familiarity with JS/TS integration
    		</objectives>

    		<example-projects>
    				Participants are encouraged to implement minimal but functional versions of these ideas – the goal is not to build a production-ready tool, but to demonstrate the concept. For instance, the “unit test writer” might only handle one simple function at a time, and that’s okay.

    				1. AI Chatbot for Coding Questions
    						|- A terminal-based chatbot where you can paste a snippet of code or an error message
    						|- the AI explains it or suggests a fix.
             	2. Smart Snippet Generator
    						|- takes a plain English request (e.g. “I need a function to parse CSV files in Node”)
    						|- returns a code snippet fulfilling that request
    						|- This could run as a CLI command or even a simple web form.
             	3.	Code Commenter
    						|- injects comments/documentation into your code
    						|- You feed it a source file and it uses an AI to generate docstring comments for each function.
             	4.	Unit Test Writer
    						|- A script that, given a function’s code, uses AI to draft basic unit tests for it.
    		</example-projects>

    		<notes>
    		</notes>
        </option-1>

        <option-2>
    		<overview>
    				- Create an AI-driven GitHub Action to automate a development workflow
    						|- PR review, issue triage, documentation updates)
    				- Focus: Deeper integration of AI into existing developer tools and workflows
    						|- triggers, CI, using API keys securely)
    				- Reinforces API usage and shows AI augmenting team processes.
    		</overview>

    		<brief>
    				Build an AI-powered GitHub Action (or similar CI pipeline tool) that improves some aspect of the development workflow automatically. The idea is to have an AI “agent” run in response to repository events (pull requests, issues, pushes) and perform a useful task. Examples of what this could do include: commenting on pull requests with a code review generated by an LLM, labeling new issues with predicted categories, writing a first draft of release notes or a changelog when a release is created, or updating documentation based on comments in code. The project is open-ended as long as it runs as an automated workflow (on GitHub or a comparable platform) and involves AI generating output or decisions. We intentionally frame the brief broadly (“AI developer tool that runs as a GitHub Action”) to encourage creativity in what the action does, rather than prescribing a single outcome.
    		</brief>

    		<objectives>
    				-	Understanding event-driven AI integration: Participants learn how to trigger AI tasks based on events (e.g. PR opened). This is a step toward production usage, where AI might continuously run in the background to assist developers.
    				- Working with GitHub Actions (CI/CD): Even if participants haven’t created custom Actions before, this is a chance to learn. Since many actions are written in JavaScript (using Node), they can use their JS skills to set up the action. They will figure out how to define workflow YAML, use the GitHub Actions toolkit, and securely manage secrets (the AI API key) in a CI environment. This gently introduces some DevOps concepts but in a focused, motivating way (since it directly relates to developer productivity).
    				- Deeper prompt tuning for specific tasks: Writing a good prompt for a code review vs. writing release notes are different challenges. Participants refine prompt skills for specialized tasks. For example, if doing a PR review bot, they must prompt the LLM with the diff or code and ask for constructive feedback in a certain format. They also must consider context limits (maybe only feed the diff chunk by chunk if large). This reinforces lessons on handling larger input by splitting or summarizing – bridging to skills they’ll need in later projects (like Project 3’s long documents).
    				- AI output validation: Since this action will run unattended, teams should think about trust and validation. For instance, if an AI labels issues, how to ensure it’s correct? We’ll likely see simple heuristic checks or confidence thresholds being discussed. This introduces the notion of AI quality control, which will be an ongoing theme (leading up to responsible AI discussions).
    		</objectives>

    		<example-projects>
    				- AI Code Reviewer: A GitHub Action that triggers on pull request creation. It fetches the diff or changed files and uses an AI to comment on potential bugs, style issues, or missing tests. (This is akin to an AI assistant for code review.) While “Build a PR code reviewer” was given as a bad example of a narrow brief, here we allow it as one option under a broader umbrella. The difference is that teams could also choose other workflows, not just PR review.
    				- Issue Triage Bot: An action that runs when new issues are submitted. It reads the issue text and uses an AI to determine labels (bug/feature/question) and perhaps suggests an assignee or relevant team based on the content. It could comment with a summary or a friendly response as well.
    				- Release Notes Generator: When a new release/tag is pushed, this action compiles commit messages or PR titles since the last release and prompts an AI to draft human-readable release notes. The team would then only need to tweak the notes rather than writing from scratch.
    				- Documentation Updater: An action that scans commit diffs for changes to code that lack corresponding docs, and opens a PR with suggested documentation changes generated by AI. (This one is more challenging, but a simplified version could focus just on detecting function docstrings that are missing and generating some.)
    		</example-projects>

    		<notes>
    		</notes>
        </option-2>
    </arc>

    <next-step>
        <overview>
		    - Our participants are primarily app developers, not infrastructure engineers. The course is tailored to that interest
			    |- focusing on building features rather than heavy DevOps.
			    |- When deployment or pipeline topics arose, we approach them lightly and in service of the main goal
				  |- e.g., GitHub Actions in Project 2 taught some CI concepts but in a directly relevant way
		    - We avoided diving into
			    |- Kubernetes
			    |- full ML Ops pipelines
			    |- other deep ops topics that might detract from the excitement of building AI functionalities
		    - This will hopefully keep motivation high
		    - That said, those interested in those areas have opportunities to get some experience
			    |- like optional deployment in final project
			    |- logging and monitoring aspects implicitly when testing, etc.
  		</overview>

  		<dev-ops-detail>
		    - The plan minimizes heavy DevOps work
			    |- deployments and MLOps pipelines are not a primary focus since participants are...
				    |- more interested in...
					    |- development
					    |- product features
				    |- ...and less motivated by...
					    |- setting up CI/CD
					    |- cloud infrastructure
		    - Where deployment is needed for a project demo (e.g. making a bot accessible)...
			    |- we favor simple solutions to avoid getting bogged down in DevOps details
			    |- Such as...
				    |- GitHub Pages
				    |- a local server
				    |- a free cloud function)
  		</dev-ops-detail>
    </next-step>
  </example-inputs>
</purpose>
